#+title: moindres carrés dans la prévision de consommation d'électricité
#+author: frederic boileau
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>

* TODO lit review of least square models
** TODO Liste des publications pertinentes [0/10]
** TODO Approches pour faire face aux contraintes
*** projection
*** trust region


* TODO Linear Algebra prereqs

* TODO julia code
** TODO my julia code
*** TODO Gauss Newton
Based on python code in
[[https://medium.com/@omyllymaki/gauss-newton-algorithm-implementation-from-scratch-55ebe56aac2e][Gauss-Newton algorithm implementation from scratch | by Ossi Myllymäki | Medium]]

#+BEGIN_SRC julia :tangle ./gaussnewton.jl :comments link
using ForwardDiff
using LinearAlgebra

struct algorithmic_parameters
    max_iter::Int
    tolerance::Float64
end


function fit(f, initguess, x, alg_param::algorithmic_parameters)

    if initguess == nothing:
        error("No initial guess was provided")
    end

    coefficients = initguess
        for k in 1:alg_param.max_iter:
            jacobian = x -> ForwardDiff.jacobian(f,x)
            residual = f(x, coefficients) -. y
            coefficients = coefficients  .- pinv(jacobian) * residual #pinv is pseudoinverse
            rmse = sqrt(sum(residual.^2))

            if rmse < alg_param.tolerance:
                return coefficients
            end

        end
end


#tests
function f(x, coefficients)
    a = coefficients
    return a[1]*x.^3 + a[2]*x.^2 + a[3]*x + a[4] + a[5]*sin(x)
end


function main()
    x = collect(range(0,stop=100,length=100))
    coefficients = [-0.001, 0.1, 0.1, 2, 15]

    y = f(x, coefficients)

    algorithmic_param = algorithmic_parameters(100,10^(-6))
    init_guess = 1000000 * rand(Float64, len(coefficients))
end

#+END_SRC

** lsqfit
file:./juliaPackages/LsqFit.jl/src/curve_fit.jl

P : parameter, abstract
R : residual, abstract
J: jacobian, abstract
W : weight , abstract array

#+BEGIN_SRC julia
struct LsqFitResult{P, R, J, W <: AbstractArray}
    param::P
    resid::R
    jacobian::J
    converged::Bool
    wt::W
end
#+END_SRC



#+BEGIN_SRC  julia
StatsBase.coef(lfr::LsqFitResult) = lfr.param
StatsBase.dof(lfr::LsqFitResult) = nobs(lfr) - length(coef(lfr))
StatsBase.nobs(lfr::LsqFitResult) = length(lfr.resid)
StatsBase.rss(lfr::LsqFitResult) = sum(abs2, lfr.resid)
StatsBase.weights(lfr::LsqFitResult) = lfr.wt
StatsBase.residuals(lfr::LsqFitResult) = lfr.resid
mse(lfr::LsqFitResult) = rss(lfr)/dof(lfr)
#+END_SRC

dof: degrees of freedom
nobs: number of independent obersations on which the model was defined
rss: residual sum of squares


#+BEGIN_SRC julia
# provide a method for those who have their own Jacobian function
function lmfit(f, g, p0::AbstractArray, wt::AbstractArray; kwargs...)
    r = f(p0)
    R = OnceDifferentiable(f, g, p0, similar(r); inplace=false)
    lmfit(R, p0, wt; kwargs...)
end
#+END_SRC

- apply f to get residuals
- OnceDifferentiable, similar
- lmfit(R, p0 wt; kwargs...)

  We have a chain of functions that result in calling:
  #+BEGIN_SRC julia
function lmfit(R::OnceDifferentiable, p0::AbstractArray, wt::AbstractArray; autodiff = :finite, kwargs...)
    results = levenberg_marquardt(R, p0; kwargs...)
    p = minimizer(results)
    return LsqFitResult(p, value!(R, p), jacobian!(R, p), converged(results), wt)
end
  #+END_SRC

- where is minimizer?
- analyze levenberg_marquardt code

*** test
:PROPERTIES:
:header-args: :tangle tests.jl
:END:
x: array of independent variables
p: array of model parameters
apply to the full dataset with @.
#+BEGIN_SRC julia :results output :session
using LsqFit
@. model(x, p) = p[1]*exp(-x*p[2])
xdata = range(0, stop=10, length=20)
ydata = model(xdata, [1.0 , 2.0]) + 0.01*randn(length(xdata))
p0 = [0.5, 0.5];
#+END_SRC

#+RESULTS:
#+begin_example
model (generic function with 1 method)
0.0:0.5263157894736842:10.0
20-element Array{Float64,1}:
  0.9933271020066862
  0.34009539314609366
  0.12174084759819304
  0.061289463674629535
  0.021993391058954904
  0.003014853710991665
  0.010389981854704487
  0.009441140905918605
  0.0011803494220172185
 -0.00405435598346164
 -0.006928645358409931
  0.013533686794186
 -0.006040572290918142
 -0.008036423298907294
  0.004210055148295414
 -0.007881953609191202
 -0.009268967098313112
  0.014162672576797122
 -0.019190093954619635
 -0.002924086242983699
2-element Array{Float64,1}:
 0.5
 0.5
#+end_example

t is a composite type (LsqFitResult), with some interesting values:
dof(fit): degrees of freedom
coef(fit): best fit parameters
fit.resid: residuals = vector of residuals
fit.jacobian: estimated Jacobian at solution
#+BEGIN_SRC julia
fit = curve_fit(model, xdata, ydata, p0)
lb = [1.1, -0.5]
ub = [1.9, Inf]
p0_bounds = [1.2, 1.2] # we have to start inside the bounds
# Optional upper and/or lower bounds on the free parameters can be passed as an argument.
# Bounded and unbouded variables can be mixed by setting `-Inf` if no lower bounds
# is to be enforced for that variable and similarly for `+Inf`
fit_bounds = curve_fit(model, xdata, ydata, p0_bounds, lower=lb, upper=ub)

# We can estimate errors on the fit parameters,
# to get standard error of each parameter:
sigma = stderror(fit)
# to get margin of error and confidence interval of each parameter at 5% significance level:
margin_of_error = margin_error(fit, 0.05)
confidence_inter = confidence_interval(fit, 0.05)

# The finite difference method is used above to approximate the Jacobian.
# Alternatively, a function which calculates it exactly can be supplied instead.
function jacobian_model(x,p)
    J = Array{Float64}(undef, length(x), length(p))
    @. J[:,1] = exp(-x*p[2])     #dmodel/dp[1]
    @. @views J[:,2] = -x*p[1]*J[:,1] #dmodel/dp[2], thanks to @views we don't allocate memory for the J[:,1] slice
    J
end
#+END_SRC


**** what is fit?
#+BEGIN_SRC julia :session
fit = curve_fit(model, jacobian_model, xdata, ydata, p0)
#+END_SRC

#+RESULTS:
*** levenbergMarquardt
file:./juliaPackages/LsqFit.jl/src/levenberg_marquardt.jl

    `levenberg_marquardt(f, g, initial_x; <keyword arguments>`

Returns the argmin over x of `sum(f(x).^2)` using the Levenberg-Marquardt
algorithm, and an estimate of the Jacobian of `f` at x.

The function `f` should take an input vector of length n and return an output
vector of length m. The function `g` is the Jacobian of f, and should return an m x
n matrix. `initial_x` is an initial guess for the solution.

Implements box constraints as described in Kanzow, Yamashita, Fukushima (2004; J
Comp & Applied Math).

- tau is trust region
- Where is the Optimizer type?

#+name: signature
#+BEGIN_SRC julia
struct LevenbergMarquardt <: Optimizer end

function levenberg_marquardt(df::OnceDifferentiable,
                             initial_x::AbstractVector{T};
                             x_tol::Real = 1e-8,
                             g_tol::Real = 1e-12,
                             maxIter::Integer = 1000,
                             lambda = T(10), tau=T(Inf),
                             lambda_increase::Real = 10.0,
                             lambda_decrease::Real = 0.1,
                             min_step_quality::Real = 1e-3,
                             good_step_quality::Real = 0.75,
                             show_trace::Bool = false,
                             lower::Vector{T} = Array{T}(undef, 0),
                             upper::Vector{T} = Array{T}(undef, 0),
                             avv!::Union{Function,Nothing,Avv} = nothing
    ) where T
#+END_SRC

#+name: parameters init
#+BEGIN_SRC julia
    # First evaluation
    value_jacobian!!(df, initial_x)

    if isfinite(tau)
        lambda = tau*maximum(jacobian(df)'*jacobian(df))
    end
#+END_SRC


#+name: check parameters
#+BEGIN_SRC julia
    ((isempty(lower) || length(lower)==length(initial_x)) && (isempty(upper) || length(upper)==length(initial_x))) ||
            throw(ArgumentError("Bounds must either be empty or of the same length as the number of parameters."))
    ((isempty(lower) || all(initial_x .>= lower)) && (isempty(upper) || all(initial_x .<= upper))) ||
            throw(ArgumentError("Initial guess must be within bounds."))
    (0 <= min_step_quality < 1) || throw(ArgumentError(" 0 <= min_step_quality < 1 must hold."))
    (0 < good_step_quality <= 1) || throw(ArgumentError(" 0 < good_step_quality <= 1 must hold."))
    (min_step_quality < good_step_quality) || throw(ArgumentError("min_step_quality < good_step_quality must hold."))
#+END_SRC

#+name: other constants
#+BEGIN_SRC julia
MAX_LAMBDA = 1e16 # minimum trust region radius
MIN_LAMBDA = 1e-16 # maximum trust region radius
MIN_DIAGONAL = 1e-6 # lower bound on values of diagonal matrix used to regularize the trust region step
#+END_SRC

#+BEGIN_SRC julia
    converged = false
    x_converged = false
    g_converged = false
    iterCt = 0
    x = copy(initial_x)
    delta_x = copy(initial_x)
    a = similar(x)

    trial_f = similar(value(df))
    residual = sum(abs2, value(df))
#+END_SRC



#+name: create buffers and  an alias for the jacobian
#+BEGIN_SRC julia
    # Create buffers
    n = length(x)
    m = length(value(df))
    JJ = Matrix{T}(undef, n, n)
    n_buffer = Vector{T}(undef, n)
    Jdelta_buffer = similar(value(df))

    # and an alias for the jacobian
    J = jacobian(df)
    dir_deriv = Array{T}(undef,m)
    v = Array{T}(undef,n)
#+END_SRC

#+name: trace
#+BEGIN_SRC julia
    # Maintain a trace of the system.
    tr = OptimizationTrace{LevenbergMarquardt}()
    if show_trace
        d = Dict("lambda" => lambda)
        os = OptimizationState{LevenbergMarquardt}(iterCt, sum(abs2, value(df)), NaN, d)
        push!(tr, os)
        println(os)
    end
#+END_SRC



We want to solve:
$argmin 0.5*||J(x)*\delta_x + f(x)||^2 + \lambda*||diagm(J'* J) * \delta_x||^2$

Solving for the minimum gives:

$(J'*J + \lambda*diagm(D^t D)) * \delta_x == -J' * f(x)$, where DtD = sum(abs2, J,1)

#+name: main loop
#+BEGIN_SRC julia
    while (~converged && iterCt < maxIter)
        # jacobian! will check if x is new or not, so it is only actually
        # evaluated if x was updated last iteration.
        jacobian!(df, x) # has alias J

        # we want to solve:
        #    argmin 0.5*||J(x)*delta_x + f(x)||^2 + lambda*||diagm(J'*J)*delta_x||^2
        # Solving for the minimum gives:
        #    (J'*J + lambda*diagm(DtD)) * delta_x == -J' * f(x), where DtD = sum(abs2, J,1)
        # Where we have used the equivalence: diagm(J'*J) = diagm(sum(abs2, J,1))
        # It is additionally useful to bound the elements of DtD below to help
        # prevent "parameter evaporation".

        DtD = vec(sum(abs2, J, dims=1))
        for i in 1:length(DtD)
            if DtD[i] <= MIN_DIAGONAL
                DtD[i] = MIN_DIAGONAL
            end
        end

        # delta_x = ( J'*J + lambda * Diagonal(DtD) ) \ ( -J'*value(df) )
        mul!(JJ, transpose(J), J)
        @simd for i in 1:n
            @inbounds JJ[i, i] += lambda * DtD[i]
        end
        #n_buffer is delta C, JJ is g compared to Mark's code
        mul!(n_buffer, transpose(J), value(df))
        rmul!(n_buffer, -1)

        v .= JJ \ n_buffer


        if avv! != nothing
            #GEODESIC ACCELERATION PART
            avv!(dir_deriv, x, v)
            mul!(a, transpose(J), dir_deriv)
            rmul!(a, -1) #we multiply by -1 before the decomposition/division
            LAPACK.potrf!('U', JJ) #in place cholesky decomposition
            LAPACK.potrs!('U', JJ, a) #divides a by JJ, taking into account the fact that JJ is now the `U` cholesky decoposition of what it was before
            rmul!(a, 0.5)
            delta_x .= v .+ a
            #end of the GEODESIC ACCELERATION PART
        else
            delta_x = v
        end





        # apply box constraints
        if !isempty(lower)
            @simd for i in 1:n
               @inbounds delta_x[i] = max(x[i] + delta_x[i], lower[i]) - x[i]
            end
        end
        if !isempty(upper)
            @simd for i in 1:n
               @inbounds delta_x[i] = min(x[i] + delta_x[i], upper[i]) - x[i]
            end
        end

        # if the linear assumption is valid, our new residual should be:
        mul!(Jdelta_buffer, J, delta_x)
        Jdelta_buffer .= Jdelta_buffer .+ value(df)
        predicted_residual = sum(abs2, Jdelta_buffer)

        # try the step and compute its quality
        # compute it inplace according to NLSolversBase value(obj, cache, state)
        # interface. No bang (!) because it doesn't update df besides mutating
        # the number of f_calls

        # re-use n_buffer
        n_buffer .= x .+ delta_x
        value(df, trial_f, n_buffer)

        # update the sum of squares
        trial_residual = sum(abs2, trial_f)

        # step quality = residual change / predicted residual change
        rho = (trial_residual - residual) / (predicted_residual - residual)
        if rho > min_step_quality
            # apply the step to x - n_buffer is ready to be used by the delta_x
            # calculations after this step.
            x .= n_buffer
            # There should be an update_x_value to do this safely
            copyto!(df.x_f, x)
            copyto!(value(df), trial_f)
            residual = trial_residual
            if rho > good_step_quality
                # increase trust region radius
                lambda = max(lambda_decrease*lambda, MIN_LAMBDA)
            end
        else
            # decrease trust region radius
            lambda = min(lambda_increase*lambda, MAX_LAMBDA)
        end

        iterCt += 1

        # show state
        if show_trace
            g_norm = norm(J' * value(df), Inf)
            d = Dict("g(x)" => g_norm, "dx" => delta_x, "lambda" => lambda)
            os = OptimizationState{LevenbergMarquardt}(iterCt, sum(abs2, value(df)), g_norm, d)
            push!(tr, os)
            println(os)
        end

        # check convergence criteria:
        # 1. Small gradient: norm(J^T * value(df), Inf) < g_tol
        # 2. Small step size: norm(delta_x) < x_tol
        if norm(J' * value(df), Inf) < g_tol
            g_converged = true
        end
        if norm(delta_x) < x_tol*(x_tol + norm(x))
            x_converged = true
        end
        converged = g_converged | x_converged
    end

    MultivariateOptimizationResults(
        LevenbergMarquardt(),    # method
        initial_x,             # initial_x
        x,                     # minimizer
        sum(abs2, value(df)),       # minimum
        iterCt,                # iterations
        !converged,            # iteration_converged
        x_converged,           # x_converged
        0.0,                   # x_tol
        0.0,
        false,                 # f_converged
        0.0,                   # f_tol
        0.0,
        g_converged,           # g_converged
        g_tol,                  # g_tol
        0.0,
        false,                 # f_increased
        tr,                    # trace
        first(df.f_calls),               # f_calls
        first(df.df_calls),               # g_calls
        0                      # h_calls
    )
end
#+END_SRC
*** unkown types
- Optimizer
- OnceDifferentiable

** statsbase
file:./juliaPackages/StatsBase.jl/src/statmodels.jl


* python gauss newton
Code taken from :
[[https://medium.com/@omyllymaki/gauss-newton-algorithm-implementation-from-scratch-55ebe56aac2e][Gauss-Newton algorithm implementation from scratch | by Ossi Myllymäki | Medium]]

Given response vector y, dependent variable x and fit function f
minimize sum(residual^2) where residual =f(x,coefficients) - y

#+BEGIN_SRC python :tangle ./gaussnewton.py :comments link
import logging
from typing import Callable
import numpy as np
from numpy.linalg import pinv

logger = logging.getLogger(__name__)

class GNSolver:
    """
    Gauss-Newton solver

    Given response vector y, dependent variable x
    and fit function f Minimize sum(residual^2)
    where residual = f(x, coefficients) - y
    """

    def __init__(self,
                 fit_function: Callable,
                 max_iter: int = 1000,
                 tolerance_difference: float = 10 ** (-16),
                 tolerance:float = 10 ** (-9),
                 init_guess: np.ndarray = None):
        self.fit_function = fit_function
        self.max_iter = max_iter
        self.tolerance_difference = tolerance_difference
        self.tolerance = tolerance
        self.coefficients = None
        self.x = None
        self.y = None
        self.init_guess = None
        if init_guess is not None:
            self.init_guess = init_guess

    def fit(self,
            x: np.ndarray,
            y: np.ndarray,
            init_guess: np.ndarray = None) -> np.ndarray:
        """
        Fit coefficients by minimizing RSE
        """
        self.x = x
        self.y = y
        if init_guess is not None:
            self.init_guess = init_guess

        if init_guess is None:
            raise Exception("Initial guess needs to be provided")

        self.coefficients = self.init_guess
        rmse_prev = np.inf
        for k in range(self.max_iter):

            residual = self.get_residual()
            jacobian = self._calculate_jacobian(self.coefficients, step=10 ** (-6))
            self.coefficients = self.coefficients - self._calculate_pseudinverse(jacobian) @ residual
            rmse = np.sqrt(np.sum(residual ** 2))
            logger.info(f"Round {k}: RMSE {rmse}")

            if self.tolerance_difference is not None:
                diff = np.abs(rms_prev - rmse)
                if diff < self.tolerance_difference:
                    logger.info(
                        "RMSE difference between iterations smaller than tolerance. Fit terminated")
                    return self.coefficients
            if rmse < self.tolerance:
                logger.info("RMSE error smaller than tolerance. Fit terminated.")
                return self.coefficients

            rmse_prev = rmse

        logger.info("Max number of iterations reached. Fit didn't converge")

        return self.coefficients

    def predict(self, x: np.ndarray):
        return self.fit_function(x, self.coefficients)

    def get_residual(self) -> np.ndarray:
        return self._calculate_residual(self.coefficients)

    def get_estimate(self) -> np.ndarray:
        return self.fit_function(self.x, self.coefficients)

    def _calculate_residual(self, coefficients: np.ndarray) -> np.ndarray:
        y_fit = self.fit_function(self.x, coefficients)
        return y_fit - self.y

    def _calculate_jacobian(self,x0):
        pass

    @staticmethod
    def _calculate_pseudoinverse(x: np.ndarray) -> np.ndarray
#+END_SRC


* fortran code enlsip
** variables
FEX65 and HEX65 are user written subroutines
/BACK/ is name of common block for variables :
BESTRK, BESTPGG, NRREST, LATTRY

#+BEGIN_SRC fortran
    EXTERNAL FEX65,HEX65
    INTEGER N,M,P,L
    INTEGER EXIT
    INTEGER ACTIVE(10),WINT(36)
    DOUBLE PRECISION
    *     H(7),X(3),F(10),WREAL(136)
    DOUBLE PRECISION
    *    BESTRK,BESTPG
    INTEGER NRREST,LATTRY
    COMMON /BACK/ BESTRK,BESTPG, NRREST,LATTRY
    MDI=36
    MDR=136
    NOUT=10
    N=3
    M=3
    P=0
    L=7
#+END_SRC


* data

* publishing options
#+BEGIN_SRC emacs-lisp
(setq org-publish-project-alist
      '(("memoir-org"
         :base-directory "~/memoire/src"
         :base-extension "org"
         :publishing-directory "~/memoire/docs"
         :recursive t
         :exclude "*/ignore/*"
         :publishing-function org-html-publish-to-html
         :headline-levels 4             ; Just the default for this project.
         :auto-preamble t)

        ("memoir-static"
         :base-directory "~/memoire/static"
         :base-extension "jl\\|css\\|js\\|png\\|jpg\\|gif\\|pdf\\|mp3\\|ogg\\|swf"
         :publishing-directory "~/memoire/docs"
         :recursive t
         :publishing-function org-publish-attachment)

        ("memoir" :components ("memoir-org" "memoir-static"))))
#+END_SRC

#+RESULTS:
| memoir-org    | :base-directory | ~/memoire                  | :base-extension | org  | :publishing-directory | ~/memoire/docs | :recursive | t    | :exclude | */ignore/* | :publishing-function | org-html-publish-to-html | :headline-levels      |              4 | :auto-preamble | t |                      |                        |
| memoir-static | :base-directory | ~/memoire/                 | :base-extension | css\ | js\                   | png\           | jpg\       | gif\ | pdf\     | mp3\       | ogg\                 | swf                      | :publishing-directory | ~/memoire/docs | :recursive     | t | :publishing-function | org-publish-attachment |
| memoir        | :components     | (memoir-org memoir-static) |                 |      |                       |                |            |      |          |            |                      |                          |                       |                |                |   |                      |                        |
